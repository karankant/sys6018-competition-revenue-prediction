Y.train = Boston[train, 14]
Y.test = Boston[-train, 14]
p = dim(Boston)[2] - 1
p.2 = p/2
p.sq = sqrt(p)
rf.boston1 =randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, mtry = p, ntree = 500)
rf.boston2 =randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, mtry = p.2, ntree = 500)
rf.boston3 =randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, mtry = p.sq, ntree = 500)
rf.boston4 =randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, mtry = p.sq, ntree = 100)
rf.boston5 =randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, mtry = p.sq, ntree = 1000)
plot(1:500, rf.boston1$test$mse, col = "green", type = "l", xlab = "Number of Trees", ylab = "Test MSE", ylim = c(10, 19))
lines(1:500, rf.boston2$test$mse, col = "red", type = "l")
lines(1:500, rf.boston3$test$mse, col = "blue", type = "l")
lines(1:100, rf.boston4$test$mse, col = "black", type = "l")
lines(1:1000, rf.boston5$test$mse, col = "pink", type = "l")
legend("topright", c("m=p", "m=p/2", "m=sqrt(p)",'ntree=100','ntree=1000'), col = c("green", "red", "blue",'black','pink'),
cex = 1, lty = 1)
#---------Problem 8--------------#
set.seed(94)
library(ISLR)
attach(Carseats)
library(tree)
#---------8a-----------#
train=train = sample (1: nrow(Carseats ), nrow(Carseats )/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
#---------------8b--------------------#
tree.carseats=tree(Sales ~ ., data = Carseats.train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
#there is a test MSE of about 4.98
#---------------8c------------#
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
#using 15 subtrees as best CV error rate
prune.carseats = prune.tree(tree.carseats, best = 15)
par(mfrow = c(1, 1))
plot(prune.carseats)
text(prune.carseats, pretty = 0)
pred.pruned = predict(prune.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
#pruning has decreased the MSE to 4.8, so it seems to have slightly lowered from the unpruned tree
#-----------8d-------------#
library(randomForest)
bag.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = T)
bag.pred = predict(bag.carseats, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
#test MSE has decreased to 2.41, indicating that bagging has worked
importance(bag.carseats)
#CompPrice, ShelveLoc and Price are the most important variables
#--------8e------------#
rf.carseats1 = randomForest(Sales ~ ., data = Carseats.train, mtry = 4, importance = T)
rf.carseats2 = randomForest(Sales ~ ., data = Carseats.train, mtry = 5, importance = T)
rf.carseats3 = randomForest(Sales ~ ., data = Carseats.train, mtry = 6, importance = T)
rf.pred1 = predict(rf.carseats1, Carseats.test)
rf.pred2 = predict(rf.carseats2, Carseats.test)
rf.pred3 = predict(rf.carseats3, Carseats.test)
mean((Carseats.test$Sales - rf.pred1)^2)
mean((Carseats.test$Sales - rf.pred2)^2)
mean((Carseats.test$Sales - rf.pred3)^2)
importance(rf.carseats3)
#test MSE of 2.723,2.62 and 2.5 for mtry of 4,5, and 6. The most important predictors are shelveloc, price and CompPrice
#-------------PROBLEM 9-------------------#
set.seed(94)
library(ISLR)
attach(OJ)
#------9a-----------#
train = sample(nrow(OJ), nrow(OJ)/2)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
#------9b----------#
oj.tree = tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)
#There are 11 terminal noads with a error rate of 0.172.
#-------9c--------#
oj.tree
#node 4: If LoyalCH is less than 0.142213. There are 64 points below this. There is a 96.9% chance that the points
# will havethe MM classifier while 3.1% will have CH.
#--------9d--------#
plot(oj.tree)
text(oj.tree,pretty=0)
#The classifications depend on LoyalCH, PriceDiff, Store and WeekofPurchase. If LoyalCH is less than 0.1422 then the classification
# is MM. If LoyalCH>0.74912 then the classification is CH.
#---------9e---------#
oj.pred = predict(oj.tree, OJ.test, type = "class")
table(OJ.test$Purchase, oj.pred)
#the test error rate is (34+63)/(174+264+34+63)= 18.1%
#--------9f--------#
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
#-------9g----------#
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
#--------9h--------#
#lowest CV error rate at 5
#-----9i-------#
oj.prune = prune.tree(oj.tree, best = 5)
plot(oj.prune)
text(oj.prune,pretty=0)
#------9j------#
summary(oj.prune)
#training error rate of 0.2037 is higher than the non-pruned rate
#------9k--------#
pred.prune = predict(oj.prune, OJ.test, type = "class")
misclass.pruned = sum(OJ.test$Purchase != pred.prune)
misclass.pruned/length(pred.pruned)
#test error rate of 52% for pruned tree compared to 20% for unpruned tree
#------------------PROBLEM 10-------------------#
set.seed(11)
library(ISLR)
attach(Hitters)
#---------------10a----------------#
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
Hitters$Salary = log(Hitters$Salary)
#----------10b-----------#
train = 1:200
hitters.train = Hitters[train, ]
hitters.test = Hitters[-train, ]
#--------10c-------------#
library(gbm)
power = seq(-10, -0.1, by = 0.2)
lambda = 10^power
length.lambda = length(lambda)
train.error = rep(0, length.lambda)
test.error = rep(0, length.lambda)
for (i in 1:length.lambda) {
boost = gbm(Salary ~ ., data = hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambda[i])
train.pred = predict(boost, hitters.train, n.trees = 1000)
test.pred = predict(boost, hitters.test, n.trees = 1000)
train.error[i] = mean((hitters.train$Salary - train.pred)^2)
test.error[i] = mean((hitters.test$Salary - test.pred)^2)
}
plot(lambda, train.error, type = "b", xlab = "Shrinkage", ylab = "Train MSE",
col = "blue", pch = 20)
#-----10d------------#
plot(lambda, test.error, type = "b", xlab = "Shrinkage", ylab = "Test MSE", col = "blue", pch = 20)
min(test.error)
#Min test MSE of 0.256 at lambda=0.1
#-------10e---------#
lm.fit = lm(Salary ~ ., data = hitters.train)
lm.pred = predict(lm.fit, hitters.test)
mean((hitters.test$Salary - lm.pred)^2)
#0.49 MSE through linear regression
library(glmnet)
x = model.matrix(Salary ~ ., data = hitters.train)
y = hitters.train$Salary
x.test = model.matrix(Salary ~ ., data = hitters.test)
lasso.fit = glmnet(x, y, alpha = 1)
lasso.pred = predict(lasso.fit, s = 0.01, newx = x.test)
mean((hitters.test$Salary - lasso.pred)^2)
#0.47 MSE with Lasso regression
#both method's MSE are higher than using boosting
#---------10f----------------#
boost= gbm(Salary ~ ., data = hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.1)
summary(boost)
#CAtBat and Putouts are the most important variables
#----------10g--------------#
rf.hitters = randomForest(Salary ~ ., data = hitters.train)
rf.pred = predict(rf.hitters, hitters.test)
mean((hitters.test$Salary - rf.pred)^2)
#MSE of 0.215
mean((hitters.test$Salary - rf.pred)^2)
rf.pred = predict(rf.hitters, hitters.test)
rf.hitters = randomForest(Salary ~ ., data = hitters.train)
#------------------PROBLEM 10-------------------#
set.seed(11)
library(ISLR)
attach(Hitters)
#---------------10a----------------#
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
Hitters$Salary = log(Hitters$Salary)
#------------------PROBLEM 10-------------------#
set.seed(11)
library(ISLR)
attach(Hitters)
#---------------10a----------------#
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
Hitters$Salary = log(Hitters$Salary)
train = 1:200
hitters.train = Hitters[train, ]
hitters.test = Hitters[-train, ]
#--------10c-------------#
library(gbm)
power = seq(-10, -0.1, by = 0.2)
lambda = 10^power
length.lambda = length(lambda)
train.error = rep(0, length.lambda)
test.error = rep(0, length.lambda)
for (i in 1:length.lambda) {
boost = gbm(Salary ~ ., data = hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambda[i])
train.pred = predict(boost, hitters.train, n.trees = 1000)
test.pred = predict(boost, hitters.test, n.trees = 1000)
train.error[i] = mean((hitters.train$Salary - train.pred)^2)
test.error[i] = mean((hitters.test$Salary - test.pred)^2)
}
plot(lambda, train.error, type = "b", xlab = "Shrinkage", ylab = "Train MSE",
col = "blue", pch = 20)
#-----10d------------#
plot(lambda, test.error, type = "b", xlab = "Shrinkage", ylab = "Test MSE", col = "blue", pch = 20)
min(test.error)
#Min test MSE of 0.256 at lambda=0.1
#-------10e---------#
lm.fit = lm(Salary ~ ., data = hitters.train)
lm.pred = predict(lm.fit, hitters.test)
mean((hitters.test$Salary - lm.pred)^2)
#0.49 MSE through linear regression
library(glmnet)
x = model.matrix(Salary ~ ., data = hitters.train)
y = hitters.train$Salary
x.test = model.matrix(Salary ~ ., data = hitters.test)
lasso.fit = glmnet(x, y, alpha = 1)
lasso.pred = predict(lasso.fit, s = 0.01, newx = x.test)
mean((hitters.test$Salary - lasso.pred)^2)
#0.47 MSE with Lasso regression
#both method's MSE are higher than using boosting
#---------10f----------------#
boost= gbm(Salary ~ ., data = hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.1)
summary(boost)
#CAtBat and Putouts are the most important variables
#----------10g--------------#
rf.hitters = randomForest(Salary ~ ., data = hitters.train)
rf.pred = predict(rf.hitters, hitters.test)
mean((hitters.test$Salary - rf.pred)^2)
y.scl<-(y1 - mean(y1)) / sd(y1)
y <- c(15.9, 7.4, 18.4, 5.6, 14.1, 13.3, 20.1, 8.9, 16.6, 16.9, 12.4, 18.4, 15.7, 16.4, 15.8, 13.6, 9.3, 8.6, 11.9, 11.5)
x1 <- c(22.0, 11.2, 19.6, 14.4, 16.0, 18.4, 23.6, 14.0, 18.8, 21.6, 22.4, 21.6, 24.4, 20.0, 20.8, 18.8, 19.2, 26.0, 27.2, 20.4)
x2 <- c(68.0, 23.0, 53.0, 26.0, 33.5, 36.5, 50.0, 26.0, 45.5, 63.5, 68.0, 53.0, 62.0, 47.0, 54.5, 41.0, 65.0, 62.0, 71.0, 50.0)
y.scl<-(y - mean(y)) / sd(y)
x1.scl <- (x1 - mean(x1)) / sd(x1)
x2.scl <- (x2 - mean(x2)) / sd(x2)
# and replace x1^2, x2^2, and x1*x2 with x1.scl^2,       |
# x2.scl^2, and x1.scl*x2.scl. For which regression      |
# approaches (among least squares, ridge regression, and |
# principle components regression) do the mean response  |
# estimates change between the original and unscaled     |
# versions?                                              |
#                                                        |
x1.scl.2 <- x1.scl^2
x2.scl.2 <- x2.scl^2
x1.x2.scl <- x1.scl*x2.scl
lm.normal = lm(y~x1.scl+x2.scl + x1.scl.2 + x2.scl.2 + x1.x2.scl)
summary(lm.normal)
x0.A <- c(21.1752, 64.8024)
x0.A.scl=0
x0.A.scl[1]<-(x0.A[1]-mean(x1)/sd(x1))
x0.A.scl[2]<-(x0.A[2]-mean(x2)/sd(x2))
x0.B <- c(11.2000, 23.0000)
x0.B.scl=0
x0.B.scl[1]<-(x0.B[1]-mean(x1)/sd(x1))
x0.B.scl[2]<-(x0.B[2]-mean(x2)/sd(x2))
x0.C <- c(11.2000, 71.0000)
x0.C.scl=0
x0.C.scl[1]<-(x0.C[1]-mean(x1)/sd(x1))
x0.C.scl[2]<-(x0.C[2]-mean(x2)/sd(x2))
x0.D <- c(27.2000, 23.0000)
x0.D.scl=0
x0.D.scl[1]<-(x0.D[1]-mean(x1)/sd(x1))
x0.D.scl[2]<-(x0.D[2]-mean(x2)/sd(x2))
x0.E <- c(27.2000, 71.0000)
x0.E.scl=0
x0.E.scl[1]<-(x0.E[1]-mean(x1)/sd(x1))
x0.E.scl[2]<-(x0.E[2]-mean(x2)/sd(x2))
x0.A.scl<- as.data.frame(t(x0.A))
x0.B.scl<- as.data.frame(t(x0.B))
x0.C.scl<- as.data.frame(t(x0.C))
x0.D.scl<- as.data.frame(t(x0.D))
x0.E.scl<- as.data.frame(t(x0.E))
a.x1.quad <- (x0.A.scl$V1)^2
a.x2.quad <- (x0.A.scl$V2)^2
a.x1.x2 <- (x0.A.scl$V1 * x0.A.scl$V2)
x0.A <- cbind(x0.A.scl, a.x1.quad, a.x2.quad, a.x1.x2)
a.x1.quad <- (x0.B.scl$V1)^2
a.x2.quad <- (x0.B.scl$V2)^2
a.x1.x2 <- (x0.B.scl$V1 * x0.B.scl$V2)
x0.B <- cbind(x0.B.scl, b.x1.quad, b.x2.quad, b.x1.x2)
x0.A <- c(21.1752, 64.8024)
x0.A.scl=0
x0.A.scl[1]<-(x0.A[1]-mean(x1)/sd(x1))
x0.A.scl[2]<-(x0.A[2]-mean(x2)/sd(x2))
x0.B <- c(11.2000, 23.0000)
x0.B.scl=0
x0.B.scl[1]<-(x0.B[1]-mean(x1)/sd(x1))
x0.B.scl[2]<-(x0.B[2]-mean(x2)/sd(x2))
x0.C <- c(11.2000, 71.0000)
x0.C.scl=0
x0.C.scl[1]<-(x0.C[1]-mean(x1)/sd(x1))
x0.C.scl[2]<-(x0.C[2]-mean(x2)/sd(x2))
x0.D <- c(27.2000, 23.0000)
x0.D.scl=0
x0.D.scl[1]<-(x0.D[1]-mean(x1)/sd(x1))
x0.D.scl[2]<-(x0.D[2]-mean(x2)/sd(x2))
x0.E <- c(27.2000, 71.0000)
x0.E.scl=0
x0.E.scl[1]<-(x0.E[1]-mean(x1)/sd(x1))
x0.E.scl[2]<-(x0.E[2]-mean(x2)/sd(x2))
x0.A.scl<- as.data.frame(t(x0.A))
x0.B.scl<- as.data.frame(t(x0.B))
x0.C.scl<- as.data.frame(t(x0.C))
x0.D.scl<- as.data.frame(t(x0.D))
x0.E.scl<- as.data.frame(t(x0.E))
a.x1.quad <- (x0.A.scl$V1)^2
a.x2.quad <- (x0.A.scl$V2)^2
a.x1.x2 <- (x0.A.scl$V1 * x0.A.scl$V2)
x0.A <- cbind(x0.A.scl, a.x1.quad, a.x2.quad, a.x1.x2)
b.x1.quad <- (x0.B.scl$V1)^2
b.x2.quad <- (x0.B.scl$V2)^2
a.x1.x2 <- (x0.B.scl$V1 * x0.B.scl$V2)
x0.B <- cbind(x0.B.scl, b.x1.quad, b.x2.quad, b.x1.x2)
c.x1.quad <- (x0.C.scl$V1)^2
c.x2.quad <- (x0.C.scl$V2)^2
c.x1.x2 <- (x0.C.scl$V1 * x0.C.scl$V2)
x0.C <- cbind(x0.C.scl, c.x1.quad, c.x2.quad, c.x1.x2)
d.x1.quad <- (x0.D.scl$V1)^2
d.x2.quad <- (x0.D.scl$V2)^2
d.x1.x2 <- (x0.D.scl$V1 * x0.D.scl$V2)
x0.D <- cbind(x0.D.scl, d.x1.quad, d.x2.quad, d.x1.x2)
e.x1.quad <- (x0.E.scl$V1)^2
e.x2.quad <- (x0.E.scl$V2)^2
e.x1.x2 <- (x0.E.scl$V1 * x0.E.scl$V2)
x0.E <- cbind(x0.E.scl, e.x1.quad, e.x2.quad, e.x1.x2)
x0.A <- c(21.1752, 64.8024)
x0.A.scl=0
x0.A.scl[1]<-(x0.A[1]-mean(x1)/sd(x1))
x0.A.scl[2]<-(x0.A[2]-mean(x2)/sd(x2))
x0.B <- c(11.2000, 23.0000)
x0.B.scl=0
x0.B.scl[1]<-(x0.B[1]-mean(x1)/sd(x1))
x0.B.scl[2]<-(x0.B[2]-mean(x2)/sd(x2))
x0.C <- c(11.2000, 71.0000)
x0.C.scl=0
x0.C.scl[1]<-(x0.C[1]-mean(x1)/sd(x1))
x0.C.scl[2]<-(x0.C[2]-mean(x2)/sd(x2))
x0.D <- c(27.2000, 23.0000)
x0.D.scl=0
x0.D.scl[1]<-(x0.D[1]-mean(x1)/sd(x1))
x0.D.scl[2]<-(x0.D[2]-mean(x2)/sd(x2))
x0.E <- c(27.2000, 71.0000)
x0.E.scl=0
x0.E.scl[1]<-(x0.E[1]-mean(x1)/sd(x1))
x0.E.scl[2]<-(x0.E[2]-mean(x2)/sd(x2))
x0.A.scl<- as.data.frame(t(x0.A))
x0.B.scl<- as.data.frame(t(x0.B))
x0.C.scl<- as.data.frame(t(x0.C))
x0.D.scl<- as.data.frame(t(x0.D))
x0.E.scl<- as.data.frame(t(x0.E))
a.x1.quad <- (x0.A.scl$V1)^2
a.x2.quad <- (x0.A.scl$V2)^2
a.x1.x2 <- (x0.A.scl$V1 * x0.A.scl$V2)
x0.A <- cbind(x0.A.scl, a.x1.quad, a.x2.quad, a.x1.x2)
b.x1.quad <- (x0.B.scl$V1)^2
b.x2.quad <- (x0.B.scl$V2)^2
b.x1.x2 <- (x0.B.scl$V1 * x0.B.scl$V2)
x0.B <- cbind(x0.B.scl, b.x1.quad, b.x2.quad, b.x1.x2)
c.x1.quad <- (x0.C.scl$V1)^2
c.x2.quad <- (x0.C.scl$V2)^2
c.x1.x2 <- (x0.C.scl$V1 * x0.C.scl$V2)
x0.C <- cbind(x0.C.scl, c.x1.quad, c.x2.quad, c.x1.x2)
d.x1.quad <- (x0.D.scl$V1)^2
d.x2.quad <- (x0.D.scl$V2)^2
d.x1.x2 <- (x0.D.scl$V1 * x0.D.scl$V2)
x0.D <- cbind(x0.D.scl, d.x1.quad, d.x2.quad, d.x1.x2)
e.x1.quad <- (x0.E.scl$V1)^2
e.x2.quad <- (x0.E.scl$V2)^2
e.x1.x2 <- (x0.E.scl$V1 * x0.E.scl$V2)
x0.E <- cbind(x0.E.scl, e.x1.quad, e.x2.quad, e.x1.x2)
colnames(x0.A) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.B) <-  c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.C) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.D) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.E) <-  c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
meanA = predict(lm.normal, newdata = x0.A)
meanB = predict(lm.normal, newdata = x0.B)
meanC = predict(lm.normal, newdata = x0.C)
meanD = predict(lm.normal, newdata = x0.D)
meanE = predict(lm.normal, newdata = x0.E)
cbind(meanA, meanB, meanC, meanD, meanE)
y.scl<-(y - mean(y)) / sd(y)
x1.scl <- (x1 - mean(x1)) / sd(x1)
x2.scl <- (x2 - mean(x2)) / sd(x2)
# and replace x1^2, x2^2, and x1*x2 with x1.scl^2,       |
# x2.scl^2, and x1.scl*x2.scl. For which regression      |
# approaches (among least squares, ridge regression, and |
# principle components regression) do the mean response  |
# estimates change between the original and unscaled     |
# versions?                                              |
#                                                        |
x1.scl.2 <- x1.scl^2
x2.scl.2 <- x2.scl^2
x1.x2.scl <- x1.scl*x2.scl
lm.normal = lm(y.scl~x1.scl+x2.scl + x1.scl.2 + x2.scl.2 + x1.x2.scl)
summary(lm.normal)
x0.A <- c(21.1752, 64.8024)
x0.A.scl=0
x0.A.scl[1]<-(x0.A[1]-mean(x1)/sd(x1))
x0.A.scl[2]<-(x0.A[2]-mean(x2)/sd(x2))
x0.B <- c(11.2000, 23.0000)
x0.B.scl=0
x0.B.scl[1]<-(x0.B[1]-mean(x1)/sd(x1))
x0.B.scl[2]<-(x0.B[2]-mean(x2)/sd(x2))
x0.C <- c(11.2000, 71.0000)
x0.C.scl=0
x0.C.scl[1]<-(x0.C[1]-mean(x1)/sd(x1))
x0.C.scl[2]<-(x0.C[2]-mean(x2)/sd(x2))
x0.D <- c(27.2000, 23.0000)
x0.D.scl=0
x0.D.scl[1]<-(x0.D[1]-mean(x1)/sd(x1))
x0.D.scl[2]<-(x0.D[2]-mean(x2)/sd(x2))
x0.E <- c(27.2000, 71.0000)
x0.E.scl=0
x0.E.scl[1]<-(x0.E[1]-mean(x1)/sd(x1))
x0.E.scl[2]<-(x0.E[2]-mean(x2)/sd(x2))
x0.A.scl<- as.data.frame(t(x0.A))
x0.B.scl<- as.data.frame(t(x0.B))
x0.C.scl<- as.data.frame(t(x0.C))
x0.D.scl<- as.data.frame(t(x0.D))
x0.E.scl<- as.data.frame(t(x0.E))
a.x1.quad <- (x0.A.scl$V1)^2
a.x2.quad <- (x0.A.scl$V2)^2
a.x1.x2 <- (x0.A.scl$V1 * x0.A.scl$V2)
x0.A <- cbind(x0.A.scl, a.x1.quad, a.x2.quad, a.x1.x2)
b.x1.quad <- (x0.B.scl$V1)^2
b.x2.quad <- (x0.B.scl$V2)^2
b.x1.x2 <- (x0.B.scl$V1 * x0.B.scl$V2)
x0.B <- cbind(x0.B.scl, b.x1.quad, b.x2.quad, b.x1.x2)
c.x1.quad <- (x0.C.scl$V1)^2
c.x2.quad <- (x0.C.scl$V2)^2
c.x1.x2 <- (x0.C.scl$V1 * x0.C.scl$V2)
x0.C <- cbind(x0.C.scl, c.x1.quad, c.x2.quad, c.x1.x2)
d.x1.quad <- (x0.D.scl$V1)^2
d.x2.quad <- (x0.D.scl$V2)^2
d.x1.x2 <- (x0.D.scl$V1 * x0.D.scl$V2)
x0.D <- cbind(x0.D.scl, d.x1.quad, d.x2.quad, d.x1.x2)
e.x1.quad <- (x0.E.scl$V1)^2
e.x2.quad <- (x0.E.scl$V2)^2
e.x1.x2 <- (x0.E.scl$V1 * x0.E.scl$V2)
x0.E <- cbind(x0.E.scl, e.x1.quad, e.x2.quad, e.x1.x2)
colnames(x0.A) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.B) <-  c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.C) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
colnames(x0.D) <- c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
meanA = predict(lm.normal, newdata = x0.A)
colnames(x0.E) <-  c("x1.scl","x2.scl", "x1.scl.2", "x2.scl.2", "x1.x2.scl")
meanB = predict(lm.normal, newdata = x0.B)
meanC = predict(lm.normal, newdata = x0.C)
meanD = predict(lm.normal, newdata = x0.D)
meanE = predict(lm.normal, newdata = x0.E)
cbind(meanA, meanB, meanC, meanD, meanE)
y.scl<-(y - mean(y)) / sd(y)
x1.scl <- (x1 - mean(x1)) / sd(x1)
x2.scl <- (x2 - mean(x2)) / sd(x2)
# and replace x1^2, x2^2, and x1*x2 with x1.scl^2,       |
# x2.scl^2, and x1.scl*x2.scl. For which regression      |
# approaches (among least squares, ridge regression, and |
# principle components regression) do the mean response  |
# estimates change between the original and unscaled     |
# versions?                                              |
#                                                        |
x1.scl.2 <- x1.scl^2
x2.scl.2 <- x2.scl^2
x1.x2.scl <- x1.scl*x2.scl
lm.normal = lm(y.scl~x1.scl+x2.scl + x1.scl.2 + x2.scl.2 + x1.x2.scl)
summary(lm.normal)
summary(lm.normal)
coefficients(lm.normal)
b.hat<-coefficients(lm.normal)
library(caret)
library(readr)
library(tidyverse)
library(data.table)
train_control <- trainControl(method="cv", number=10)
train<-read_csv('data/train_clean.csv')
setwd("C:/Users/karan/OneDrive/Documents/UVA/Fall Semester/SYS 6018/sys6018-competition-revenue-prediction")
test<-read_csv('data/test_clean_newdataset.csv')
test<-read_csv('data/test_clean_newdataset.csv')
train<-read_csv('data/train_clean.csv')
test<-read_csv('data/test_clean_newdataset.csv')
#add day of week, hour and mo
train["day_of_week"] <- wday(train$date)
train["hour"] <- hour(as.POSIXct(train$visitStartTime, origin="1970-01-01"))
train["month"] <- month(train$date)
#removing NA values and replacing with 0
train$transactionRevenue[is.na(train$transactionRevenue)]=0
#subsetting with values where transaction value is greater than 0
train.subset<-train[train$transactionRevenue>0,]
#subsetting for a certain set of predictors with high p values
train.subset2<-train.subset[c('day_of_week','hour','month','pageviews','isMobile','transactionRevenue')]
train.subset3<--train.subset[c('day_of_week','hour','month','pageviews','transactionRevenue')]
train.subset4<-train.subset[c('day_of_week','hour','month','pageviews','isMobile','continent','transactionRevenue')]
#replace NA values with 0 or unknown
train.subset2[is.na(train.subset2$pageviews)]=0
train.subset2[is.na(train.subset2)]<-'Unknown'
train$pageviews<-as.numeric(train$pageviews)
#create model
train.lm<-lm(transactionRevenue~.,data=train.subset2)
train.lm2<-lm(transactionRevenue~.,data=train.subset3)
train.lm3<-lm(transactionRevenue~.,data=train.subset4)
summary(train.lm2)
summary(train.lm3)
cv.error1 = cv.glm(train.subset ,train.lm ,K=5)$delta[1]
library(boot)
cv.error1 = cv.glm(train.subset ,train.lm ,K=5)$delta[1]
cv.error2 = cv.glm(train.subset ,train.lm2 ,K=5)$delta[1]
cv.error3 = cv.glm(train.subset ,train.lm3 ,K=5)$delta[1]
#subsetting for a certain set of predictors with high p values
train.subset2<-train.subset[c('day_of_week','hour','month','pageviews','isMobile','transactionRevenue')]
train.subset3<--train.subset[c('day_of_week','hour','month','pageviews','transactionRevenue')]
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="ols", tuneGrid=grid)
#cross-validate
cv.error1 = cv.lm(train.subset ,train.lm ,K=5)$delta[1]
